// auto generated by c2z
const std = @import("std");
//const cpp = @import("cpp");

// TIDES: BEGIN MANUAL CHANGES
const Graphics = @import("../../../Graphics/Interfaces/IGraphics.zig");

const PipelineCacheFlags = u32;
const QueueType = u32;
const ResourceState = u32;
const TextureCreationFlags = u32;
const float3 = [3]f32;
const uint = u32;

const Buffer = Graphics.Buffer;
const BufferDesc = Graphics.BufferDesc;
const Cmd = Graphics.Cmd;
const Fence = Graphics.Fence;
const IndexType = Graphics.IndexType;
const IndirectDrawIndexArguments = Graphics.IndirectDrawIndexArguments;
const mat4 = anyopaque;
const PipelineCache = Graphics.PipelineCache;
const Renderer = Graphics.Renderer;
const ResourcePlacement = Graphics.ResourcePlacement;
const ResourceSizeAlign = Graphics.ResourceSizeAlign;
const Sampler = Graphics.Sampler;
const Semaphore = Graphics.Semaphore;
const Shader = Graphics.Shader;
const ShaderConstant = Graphics.ShaderConstant;
const Texture = Graphics.Texture;
const TextureDesc = Graphics.TextureDesc;
const VertexLayout = Graphics.VertexLayout;
// TIDES: END MANUAL CHANGES

extern const _1_gVertexBufferState_: *const ResourceState;
pub const gVertexBufferState = _1_gVertexBufferState_;

extern const _1_gIndexBufferState_: *const ResourceState;
pub const gIndexBufferState = _1_gIndexBufferState_;

pub const MappedMemoryRange = extern struct {
    pData: [*c]u8,
    pBuffer: [*c]Buffer,
    mOffset: u64,
    mSize: u64,
    mFlags: u32,
};

pub const TextureContainerType = extern struct {
    bits: c_int = 0,

    /// Use whatever container is designed for that platform
    /// Windows, macOS, Linux - TEXTURE_CONTAINER_DDS
    /// iOS, Android          - TEXTURE_CONTAINER_KTX
    pub const TEXTURE_CONTAINER_DEFAULT: TextureContainerType = .{ .bits = @as(c_uint, @intCast(0)) };
    /// Explicit container types
    /// .dds
    pub const TEXTURE_CONTAINER_DDS: TextureContainerType = .{ .bits = TextureContainerType.TEXTURE_CONTAINER_DEFAULT.bits + 1 };
    /// .ktx
    pub const TEXTURE_CONTAINER_KTX: TextureContainerType = .{ .bits = TextureContainerType.TEXTURE_CONTAINER_DEFAULT.bits + 2 };
    /// .gnf
    pub const TEXTURE_CONTAINER_GNF: TextureContainerType = .{ .bits = TextureContainerType.TEXTURE_CONTAINER_DEFAULT.bits + 3 };

    // pub usingnamespace cpp.FlagsMixin(TextureContainerType);
};

pub const RegisterMaterialResult = extern struct {
    bits: c_int = 0,

    pub const REGISTER_MATERIAL_SUCCESS: RegisterMaterialResult = .{ .bits = @as(c_uint, @intCast(0)) };
    pub const REGISTER_MATERIAL_BADFILE: RegisterMaterialResult = .{ .bits = @as(c_uint, @intCast(1)) };

    // pub usingnamespace cpp.FlagsMixin(RegisterMaterialResult);
};

/// MARK: - Resource Loading
pub const BufferLoadDesc = extern struct {
    ppBuffer: [*c][*c]Buffer,
    /// This must stay valid until the buffer load is not completed
    /// Use waitForToken (if a token was passed to addResource) or waitForAllResourceLoads before freeing pData
    pData: ?*const anyopaque,
    mDesc: BufferDesc,
    /// MemZero buffer
    mForceReset: bool,
    /// Optional (if user provides staging buffer memory)
    pSrcBuffer: [*c]Buffer,
    mSrcOffset: u64,
};

/// MARK: - Resource Loading
pub const TextureLoadDesc = extern struct {
    ppTexture: [*c][*c]Texture,
    __union_field1: __Union0,
    /// Filename without extension. Extension will be determined based on mContainer
    pFileName: [*c]const u8,
    pTextureData: ?*const anyopaque,
    mTextureDataSize: usize,
    /// The index of the GPU in SLI/Cross-Fire that owns this texture, or the Renderer index in unlinked mode.
    mNodeIndex: u32,
    /// Following is ignored if pDesc != NULL.  pDesc->mFlags will be considered instead.
    mCreationFlag: TextureCreationFlags,
    /// The texture file format (dds/ktx/...)
    mContainer: TextureContainerType,

    pub const __Union0 = extern union {
        __struct_field1: __Struct0,
        /// Ycbcr sampler to use when loading ycbcr texture from file
        pYcbcrSampler: [*c]Sampler,

        /// Load empty texture
        pub const __Struct0 = extern struct {
            pDesc: [*c]TextureDesc,
            /// MemZero texture
            mForceReset: bool,
        };
    };
};

pub const BufferChunk = extern struct {
    mOffset: u32,
    mSize: u32,
};

/// Structure used to sub-allocate chunks on a buffer, keeps track of free memory to handle new requests.
/// Interface to add/remove this allocator is currently private, could be made public if needed.
pub const BufferChunkAllocator = extern struct {
    pBuffer: [*c]Buffer,
    mUsedChunkCount: u32,
    mSize: u32,
    mUnusedChunks: [*c]BufferChunk,
};

/// Structure used to sub-allocate chunks on a buffer, keeps track of free memory to handle new requests.
/// Interface to add/remove this allocator is currently private, could be made public if needed.
/// Stores huge buffers that are then used to sub-allocate memory for each of the loaded meshes.
/// GeometryBuffer can be provided to GeometryLoadDesc::pGeometryBuffer when loading a mesh, sub-chunks will be allocated
/// by mIndex and mVertex allocators and return the BufferChunk(s) that where used in Geometry::mIndexBufferChunk and
/// Geometry::mVertexBufferChunks
pub const GeometryBuffer = extern struct {
    mIndex: BufferChunkAllocator,
    mVertex: [15]BufferChunkAllocator,
};

/// Stores huge buffers that are then used to sub-allocate memory for each of the loaded meshes.
/// GeometryBuffer can be provided to GeometryLoadDesc::pGeometryBuffer when loading a mesh, sub-chunks will be allocated
/// by mIndex and mVertex allocators and return the BufferChunk(s) that where used in Geometry::mIndexBufferChunk and
/// Geometry::mVertexBufferChunks
extern const _1_GEOMETRY_FILE_MAGIC_STR_: *const [10]u8;
pub const GEOMETRY_FILE_MAGIC_STR = _1_GEOMETRY_FILE_MAGIC_STR_;

pub const Meshlet = extern struct {
    /// Offsets within meshlet_vertices and meshlet_triangles arrays with meshlet data
    vertexOffset: uint,
    triangleOffset: uint,
    /// Number of vertices and triangles used in the meshlet; data is stored in consecutive range defined by offset and count
    vertexCount: uint,
    triangleCount: uint,
};

pub const MeshletData = extern struct {
    center: float3,
    radius: f32,
    /// Normal cone, useful for backface culling
    coneApex: float3,
    coneAxis: float3,
    /// = cos(angle/2)
    coneCutoff: f32,
};

pub const GeometryMeshlets = extern struct {
    mMeshletCount: u64,
    mMeshlets: [*c]Meshlet,
    mMeshletsData: [*c]MeshletData,
    mVertexCount: u64,
    mVertices: [*c]u32,
    mTriangleCount: u64,
    mTriangles: [*c]u8,
};

pub const Geometry = extern struct {
    __union_field1: __Union0,
    /// The array of traditional draw arguments to draw each subset in this geometry
    pDrawArgs: [*c]IndirectDrawIndexArguments,
    /// The array of vertex buffer strides to bind when drawing this geometry
    mVertexStrides: [15]u32,
    bitfield_1: packed struct(u32) {
        // NOTE: Bitfield generation not guaranteed to work on all platforms, use with caution.

        /// Number of vertex buffers in this geometry
        mVertexBufferCount: u8, // 8 bits
        /// Index type (32 or 16 bit)
        mIndexType: u2, // 10 bits
        /// Number of draw args in the geometry
        mDrawArgCount: u22, // 32 bits
    },

    /// Number of indices in the geometry
    mIndexCount: u32,
    /// Number of vertices in the geometry
    mVertexCount: u32,
    /// If present, data is stored in pGeometryBuffer
    pGeometryBuffer: [*c]GeometryBuffer,
    meshlets: GeometryMeshlets,
    // TIDES: BEGIN MANUAL CHANGES
    mAabbMin: [3]f32,
    mAabbMax: [3]f32,
    mAabbCenter: [3]f32,
    mRadius: f32,
    mPad: [10]u32,
    // TIDES: END MANUAL CHANGES

    pub const __Union0 = extern union {
        __struct_field1: __Struct0,
        __struct_field3: __Struct2,

        pub const __Struct0 = extern struct {
            /// Index buffer to bind when drawing this geometry
            pIndexBuffer: [*c]Buffer,
            /// The array of vertex buffers to bind when drawing this geometry
            pVertexBuffers: [15][*c]Buffer,
        };

        pub const __Struct2 = extern struct {
            /// Used when Geometry is loaded to unified GeometryBuffer object (when GeometryLoadDesc::pGeometryBuffer is valid)
            mIndexBufferChunk: BufferChunk,
            mVertexBufferChunks: [15]BufferChunk,
        };
    };
};

/// Outputs data that's only needed in the CPU side, OTOH the Geometry object holds GPU related information and buffers
pub const GeometryData = extern struct {
    /// Shadow copy of the geometry vertex and index data if requested through the load flags
    pShadow: [*c]ShadowData,
    /// The array of joint inverse bind-pose matrices ( object-space )
    pInverseBindPoses: *mat4,
    /// The array of data to remap skin batch local joint ids to global joint ids
    pJointRemaps: [*c]u32,
    /// Number of joints in the skinned geometry
    mJointCount: u32,
    /// Hair data
    mHair: Hair,
    mPad0: [1]u32,
    meshlets: [*c]MeshletData,
    /// Custom data imported by the user in custom AssetPipelines, this can be data that was exported from a custom tool/plugin
    /// specific to the engine/game. See AssetPipeline: callbacks in ProcessGLTFParams for more information.
    pUserData: ?*anyopaque,
    mUserDataSize: u32,
    mPad1: [5]u32,

    pub const Hair = extern struct {
        mVertexCountPerStrand: u32,
        mGuideCountPerStrand: u32,
    };

    pub const ShadowData = extern struct {
        pIndices: ?*anyopaque,
        pAttributes: [19]?*anyopaque,
        /// Strides for the data in pAttributes, this might not match Geometry::mVertexStrides since those are generated based on
        /// GeometryLoadDesc::pVertexLayout, e.g. if the normals are packed on the GPU as half2 then:
        ///         - Geometry::mVertexStrides will be sizeof(half2)
        ///         - ShadowData::mVertexStrides might be sizeof(float3) = 12 (or maybe sizeof(float4) = 16)
        /// If the data readed from the file in pAttributes is already packed then ShadowData::mVertexStrides[i] ==
        /// Geometry::mVertexStrides[i]
        mVertexStrides: [19]u32,
        /// We might have a different number of attributes than mVertexCount.
        /// This happens for example for Hair
        mAttributeCount: [19]u32,
    };
};

/// Outputs data that's only needed in the CPU side, OTOH the Geometry object holds GPU related information and buffers
pub const GeometryLoadFlags = extern struct {
    bits: c_int = 0,

    pub const GEOMETRY_LOAD_FLAG_NONE: GeometryLoadFlags = .{ .bits = @as(c_uint, @intCast(0)) };
    /// Keep shadow copy of indices and vertices for CPU
    pub const GEOMETRY_LOAD_FLAG_SHADOWED: GeometryLoadFlags = .{ .bits = @as(c_uint, @intCast(1)) };
    /// Use structured buffers instead of raw buffers
    pub const GEOMETRY_LOAD_FLAG_STRUCTURED_BUFFERS: GeometryLoadFlags = .{ .bits = @as(c_uint, @intCast(2)) };
    /// Geometry buffers can be used as input for ray tracing
    pub const GEOMETRY_LOAD_FLAG_RAYTRACING_INPUT: GeometryLoadFlags = .{ .bits = @as(c_uint, @intCast(4)) };

    // pub usingnamespace cpp.FlagsMixin(GeometryLoadFlags);
};

pub const GeometryBufferLoadDesc = extern struct {
    mStartState: ResourceState,
    pNameIndexBuffer: [*c]const u8,
    pNamesVertexBuffers: [15][*c]const u8,
    mIndicesSize: u32,
    mVerticesSizes: [15]u32,
    pIndicesPlacement: [*c]ResourcePlacement,
    pVerticesPlacements: [15][*c]ResourcePlacement,
    pOutGeometryBuffer: [*c][*c]GeometryBuffer,
};

pub const GeometryBufferLayoutDesc = extern struct {
    mIndexType: IndexType,
    mVerticesStrides: [15]u32,
    /// Vertex buffer/binding idx for each semantic.
    /// Used to locate attributes inside specific buffers for loaded Geometry.
    mSemanticBindings: [19]u32,
};

pub const GeometryLoadDesc = extern struct {
    /// Output geometry
    ppGeometry: [*c][*c]Geometry,
    ppGeometryData: [*c][*c]GeometryData,
    /// Filename of geometry container
    pFileName: [*c]const u8,
    /// Loading flags
    mFlags: GeometryLoadFlags,
    /// Linked gpu node / Unlinked Renderer index
    mNodeIndex: u32,
    /// Specifies how to arrange the vertex data loaded from the file into GPU memory
    pVertexLayout: [*c]const VertexLayout,
    /// Optional preallocated unified buffer for geometry.
    /// When this parameter is specified, Geometry::pDrawArgs values are going
    /// to be shifted according to index/vertex location within BufferChunkAllocator.
    pGeometryBuffer: [*c]GeometryBuffer,
    /// Used to convert data to desired state inside GeometryBuffer.
    pGeometryBufferLayoutDesc: [*c]GeometryBufferLayoutDesc,
};

pub const BufferUpdateDesc = extern struct {
    pBuffer: [*c]Buffer,
    mDstOffset: u64,
    mSize: u64,
    /// To be filled by the caller between beginUpdateResource and endUpdateResource calls
    /// Example:
    /// BufferUpdateDesc update = { pBuffer, bufferDstOffset };
    /// beginUpdateResource(
    ///&update
    ///);
    /// ParticleVertex* vertices = (ParticleVertex*)update.pMappedData;
    ///   for (uint32_t i = 0; i
    ///<
    /// particleCount; ++i)
    ///     vertices[i] = { rand() };
    /// endUpdateResource(
    ///&update
    ///,
    ///&token
    ///);
    pMappedData: ?*anyopaque,
    /// Optional (if user provides staging buffer memory)
    pSrcBuffer: [*c]Buffer,
    mSrcOffset: u64,
    mCurrentState: ResourceState,
    mInternal: __Struct0,

    /// Internal
    pub const __Struct0 = extern struct {
        mMappedRange: MappedMemoryRange,
    };
};

pub const TextureSubresourceUpdate = extern struct {
    /// Filled by ResourceLaoder in beginUpdateResource
    /// Size of each row in destination including padding - Needs to be respected otherwise texture data will be corrupted if dst row stride
    /// is not the same as src row stride
    mDstRowStride: u32,
    /// Number of rows in this slice of the texture
    mRowCount: u32,
    /// Src row stride for convenience (mRowCount * width * texture format size)
    mSrcRowStride: u32,
    /// Size of each slice in destination including padding - Use for offsetting dst data updating 3D textures
    mDstSliceStride: u32,
    /// Size of each slice in src - Use for offsetting src data when updating 3D textures
    mSrcSliceStride: u32,
    /// To be filled by the caller
    /// Example:
    /// BufferUpdateDesc update = { pTexture, 2, 1 };
    /// beginUpdateResource(
    ///&update
    ///);
    /// Row by row copy is required if mDstRowStride > mSrcRowStride. Single memcpy will work if mDstRowStride == mSrcRowStride
    /// 2D
    /// for (uint32_t r = 0; r
    ///<
    /// update.mRowCount; ++r)
    ///     memcpy(update.pMappedData + r * update.mDstRowStride, srcPixels + r * update.mSrcRowStride, update.mSrcRowStride);
    /// 3D
    /// for (uint32_t z = 0; z
    ///<
    /// depth; ++z)
    /// {
    ///     uint8_t* dstData = update.pMappedData + update.mDstSliceStride * z;
    ///     uint8_t* srcData = srcPixels + update.mSrcSliceStride * z;
    ///     for (uint32_t r = 0; r
    ///<
    /// update.mRowCount; ++r)
    ///         memcpy(dstData + r * update.mDstRowStride, srcData + r * update.mSrcRowStride, update.mSrcRowStride);
    /// }
    /// endUpdateResource(
    ///&update
    ///,
    ///&token
    ///);
    pMappedData: [*c]u8,
};

/// #NOTE: Only use for procedural textures which are created on CPU (noise textures, font texture, ...)
pub const TextureUpdateDesc = extern struct {
    pTexture: [*c]Texture,
    mBaseMipLevel: u32,
    mMipLevels: u32,
    mBaseArrayLayer: u32,
    mLayerCount: u32,
    mCurrentState: ResourceState,
    /// Optional - If we want to run the update on user specified command buffer instead
    pCmd: [*c]Cmd,
    mInternal: __Struct0,

    extern fn _1_TextureUpdateDesc_getSubresourceUpdateDesc_(self: *TextureUpdateDesc, mip: u32, layer: u32) TextureSubresourceUpdate;
    pub const getSubresourceUpdateDesc = _1_TextureUpdateDesc_getSubresourceUpdateDesc_;

    /// Internal
    pub const __Struct0 = extern struct {
        mMappedRange: MappedMemoryRange,
        mDstSliceStride: u32,
        mSkipBarrier: bool,
    };
};

/// #NOTE: Only use for procedural textures which are created on CPU (noise textures, font texture, ...)
pub const TextureCopyDesc = extern struct {
    pTexture: [*c]Texture,
    pBuffer: [*c]Buffer,
    /// Semaphore to synchronize graphics/compute operations that write to the texture with the texture -> buffer copy.
    pWaitSemaphore: [*c]Semaphore,
    mTextureMipLevel: u32,
    mTextureArrayLayer: u32,
    /// Current texture state.
    mTextureState: ResourceState,
    /// Queue the texture is copied from.
    mQueueType: QueueType,
    mBufferOffset: u64,
};

pub const ShaderStageLoadDesc = extern struct {
    pFileName: [*c]const u8,
    pEntryPointName: [*c]const u8,
};

pub const ShaderLoadDesc = extern struct {
    mVert: ShaderStageLoadDesc,
    mFrag: ShaderStageLoadDesc,
    mGeom: ShaderStageLoadDesc,
    mHull: ShaderStageLoadDesc,
    mDomain: ShaderStageLoadDesc,
    mComp: ShaderStageLoadDesc,
    mGraph: ShaderStageLoadDesc,
    mAmplification: ShaderStageLoadDesc,
    mMesh: ShaderStageLoadDesc,
    pConstants: [*c]const ShaderConstant,
    mConstantCount: u32,
};

pub const PipelineCacheLoadDesc = extern struct {
    pFileName: [*c]const u8,
    mFlags: PipelineCacheFlags,
};

pub const PipelineCacheSaveDesc = extern struct {
    pFileName: [*c]const u8,
};

pub const SyncToken = u64;

pub const ResourceLoaderDesc = extern struct {
    mBufferSize: u64,
    mBufferCount: u32,
    mSingleThreaded: bool,
    mUseMaterials: bool,
};

extern var _1_gDefaultResourceLoaderDesc_: *ResourceLoaderDesc;
pub const gDefaultResourceLoaderDesc = _1_gDefaultResourceLoaderDesc_;

extern fn _1_initResourceLoaderInterface_(pRenderer: *Renderer, pDesc: *ResourceLoaderDesc) void;
/// MARK: - Resource Loader Functions
pub fn initResourceLoaderInterface(
    pRenderer: *Renderer,
    pDesc: *ResourceLoaderDesc,
) void {
    return _1_initResourceLoaderInterface_(pRenderer, pDesc);
}

extern fn _1_exitResourceLoaderInterface_(pRenderer: *Renderer) void;
pub const exitResourceLoaderInterface = _1_exitResourceLoaderInterface_;

extern fn _2_initResourceLoaderInterface_(ppRenderers: [*c][*c]Renderer, rendererCount: u32, pDesc: [*c]ResourceLoaderDesc) void;
/// Multiple Renderer (unlinked GPU) variants. The Resource Loader must be shared between Renderers.
pub fn initResourceLoaderInterface__Overload2(
    ppRenderers: [*c][*c]Renderer,
    rendererCount: u32,
    __opt: struct {
        pDesc: [*c]ResourceLoaderDesc = null,
    },
) void {
    return _2_initResourceLoaderInterface_(ppRenderers, rendererCount, __opt.pDesc);
}

extern fn _2_exitResourceLoaderInterface_(ppRenderers: [*c][*c]Renderer, rendererCount: u32) void;
pub const exitResourceLoaderInterface__Overload2 = _2_exitResourceLoaderInterface_;

extern fn _1_addMaterial_(pMaterialFileName: [*c]const u8, pMaterial: [*c][*c]Material, pSyncToken: [*c]SyncToken) u32;
/// Will load a material and all related shaders/textures (if they are not already loaded, Material shaders/textures are shared across all
/// Materials)
pub const addMaterial = _1_addMaterial_;

extern fn _1_removeMaterial_(pMaterial: [*c]Material) void;
/// Will unload all the related shaders/textures (if they are not still used by some other Material)
pub const removeMaterial = _1_removeMaterial_;

extern fn _1_getMaterialSetIndex_(pMaterial: [*c]Material, name: [*c]const u8) u32;
/// TODO: Functions below are a simple interface to get resources from materials, as we develop materials further this interface will
/// probably change.
pub const getMaterialSetIndex = _1_getMaterialSetIndex_;

extern fn _1_getMaterialShader_(pMaterial: [*c]Material, materialSetIndex: u32, ppOutShader: [*c][*c]Shader) void;
pub const getMaterialShader = _1_getMaterialShader_;

extern fn _1_getMaterialTextures_(pMaterial: [*c]Material, materialSetIndex: u32, ppOutTextureBindingNames: [*c]const [*c]u8, ppOutTextures: [*c][*c]Texture, outTexturesSize: u32) void;
pub const getMaterialTextures = _1_getMaterialTextures_;

extern fn _1_getResourceSizeAlign_(pDesc: [*c]const BufferLoadDesc, pOut: [*c]ResourceSizeAlign) void;
/// MARK: addResource and updateResource
pub const getResourceSizeAlign = _1_getResourceSizeAlign_;

extern fn _2_getResourceSizeAlign_(pDesc: [*c]const TextureLoadDesc, pOut: [*c]ResourceSizeAlign) void;
pub const getResourceSizeAlign__Overload2 = _2_getResourceSizeAlign_;

extern fn _1_addResource_(pBufferDesc: [*c]BufferLoadDesc, token: [*c]SyncToken) void;
/// If token is NULL, the resource will be available when allResourceLoadsCompleted() returns true.
/// If token is non NULL, the resource will be available after isTokenCompleted(token) returns true.
pub const addResource = _1_addResource_;

extern fn _2_addResource_(pTextureDesc: [*c]TextureLoadDesc, token: [*c]SyncToken) void;
pub const addResource__Overload2 = _2_addResource_;

extern fn _3_addResource_(pGeomDesc: [*c]GeometryLoadDesc, token: [*c]SyncToken) void;
pub const addResource__Overload3 = _3_addResource_;

extern fn _1_addGeometryBuffer_(pDesc: [*c]GeometryBufferLoadDesc) void;
pub const addGeometryBuffer = _1_addGeometryBuffer_;

extern fn _1_beginUpdateResource_(pBufferDesc: [*c]BufferUpdateDesc) void;
pub const beginUpdateResource = _1_beginUpdateResource_;

extern fn _2_beginUpdateResource_(pTextureDesc: [*c]TextureUpdateDesc) void;
pub const beginUpdateResource__Overload2 = _2_beginUpdateResource_;

extern fn _1_endUpdateResource_(pBuffer: [*c]BufferUpdateDesc) void;
pub const endUpdateResource = _1_endUpdateResource_;

extern fn _2_endUpdateResource_(pTexture: [*c]TextureUpdateDesc) void;
pub const endUpdateResource__Overload2 = _2_endUpdateResource_;

extern fn _1_addGeometryBufferPart_(buffer: [*c]BufferChunkAllocator, size: u32, alignment: u32, pOut: [*c]BufferChunk, pPreferredChunk: [*c]BufferChunk) void;
/// This function is used to acquire geometry buffer location.
/// It can be used on index or vertex buffer
/// When there are no continious chunk with enough size, output chunk contains 0 size.
/// Use releaseGeometryBufferPart to release chunk.
/// Make sure all chunks are released before removeGeometryBuffer.
pub fn addGeometryBufferPart(
    buffer: [*c]BufferChunkAllocator,
    size: u32,
    alignment: u32,
    pOut: [*c]BufferChunk,
    __opt: struct {
        pPreferredChunk: [*c]BufferChunk = null,
    },
) void {
    return _1_addGeometryBufferPart_(buffer, size, alignment, pOut, __opt.pPreferredChunk);
}

extern fn _1_removeGeometryBufferPart_(buffer: [*c]BufferChunkAllocator, chunk: [*c]BufferChunk) void;
/// Release previously claimed chunk to buffer.
/// Buffer must be the one passed to claimGeometryBufferPart for this chunk.
pub const removeGeometryBufferPart = _1_removeGeometryBufferPart_;

pub const FlushResourceUpdateDesc = extern struct {
    mNodeIndex: u32,
    mWaitSemaphoreCount: u32,
    ppWaitSemaphores: [*c][*c]Semaphore,
    pOutFence: [*c]Fence,
    pOutSubmittedSemaphore: [*c]Semaphore,
};

extern fn _1_flushResourceUpdates_(pDesc: [*c]FlushResourceUpdateDesc) void;
pub const flushResourceUpdates = _1_flushResourceUpdates_;

extern fn _1_copyResource_(pTextureDesc: [*c]TextureCopyDesc, token: [*c]SyncToken) void;
/// Copies data from GPU to the CPU, typically for transferring it to another GPU in unlinked mode.
/// For optimal use, the amount of data to transfer should be minimized as much as possible and applications should
/// provide additional graphics/compute work that the GPU can execute alongside the copy.
pub const copyResource = _1_copyResource_;

extern fn _1_removeResource_(pBuffer: [*c]Buffer) void;
/// MARK: removeResource
pub const removeResource = _1_removeResource_;

extern fn _2_removeResource_(pTexture: [*c]Texture) void;
pub const removeResource__Overload2 = _2_removeResource_;

extern fn _3_removeResource_(pGeom: [*c]Geometry) void;
pub const removeResource__Overload3 = _3_removeResource_;

extern fn _4_removeResource_(pGeom: [*c]GeometryData) void;
pub const removeResource__Overload4 = _4_removeResource_;

extern fn _1_removeGeometryBuffer_(pGeomBuffer: [*c]GeometryBuffer) void;
pub const removeGeometryBuffer = _1_removeGeometryBuffer_;

extern fn _1_removeGeometryShadowData_(pGeom: [*c]GeometryData) void;
/// Frees pGeom->pShadow in case it was requested with GEOMETRY_LOAD_FLAG_SHADOWED and you are already done with it
pub const removeGeometryShadowData = _1_removeGeometryShadowData_;

extern fn _1_allResourceLoadsCompleted_() bool;
/// Returns whether all submitted resource loads and updates have been completed.
pub const allResourceLoadsCompleted = _1_allResourceLoadsCompleted_;

extern fn _1_waitForAllResourceLoads_() void;
/// Blocks the calling thread until allResourceLoadsCompleted() returns true.
/// Note that if more resource loads or updates are submitted from a different thread while
/// while the calling thread is blocked, those loads or updates are not guaranteed to have
/// completed when this function returns.
pub const waitForAllResourceLoads = _1_waitForAllResourceLoads_;

extern fn _1_waitCopyQueueIdle_() void;
/// Wait for the copy queue to finish all work
pub const waitCopyQueueIdle = _1_waitCopyQueueIdle_;

extern fn _1_isResourceLoaderSingleThreaded_() bool;
/// Returns wheter the resourceloader is single threaded or not
pub const isResourceLoaderSingleThreaded = _1_isResourceLoaderSingleThreaded_;

extern fn _1_getLastTokenCompleted_() SyncToken;
/// A SyncToken is an array of monotonically increasing integers.
/// getLastTokenCompleted() returns the last value for which
/// isTokenCompleted(token) is guaranteed to return true.
pub const getLastTokenCompleted = _1_getLastTokenCompleted_;

extern fn _1_isTokenCompleted_(token: [*c]const SyncToken) bool;
pub const isTokenCompleted = _1_isTokenCompleted_;

extern fn _1_waitForToken_(token: [*c]const SyncToken) void;
pub const waitForToken = _1_waitForToken_;

extern fn _1_getLastTokenSubmitted_() SyncToken;
/// Allows clients to synchronize with the submission of copy commands (as opposed to their completion).
/// This can reduce the wait time for clients but requires using the Semaphore from getLastSemaphoreCompleted() in a wait
/// operation in a submit that uses the textures just updated.
pub const getLastTokenSubmitted = _1_getLastTokenSubmitted_;

extern fn _1_isTokenSubmitted_(token: [*c]const SyncToken) bool;
pub const isTokenSubmitted = _1_isTokenSubmitted_;

extern fn _1_waitForTokenSubmitted_(token: [*c]const SyncToken) void;
pub const waitForTokenSubmitted = _1_waitForTokenSubmitted_;

extern fn _1_getLastSemaphoreSubmitted_(nodeIndex: u32) [*c]Semaphore;
/// Return the semaphore for the last copy operation of a specific GPU.
/// Could be NULL if no operations have been executed.
pub const getLastSemaphoreSubmitted = _1_getLastSemaphoreSubmitted_;

extern fn _1_addShader_(pRenderer: [*c]Renderer, pDesc: [*c]const ShaderLoadDesc, pShader: [*c][*c]Shader) void;
/// Either loads the cached shader bytecode or compiles the shader to create new bytecode depending on whether source is newer than binary
pub const addShader = _1_addShader_;

extern fn _1_loadPipelineCache_(pRenderer: [*c]Renderer, pDesc: [*c]const PipelineCacheLoadDesc, ppPipelineCache: [*c][*c]PipelineCache) void;
/// Save/Load pipeline cache from disk
pub const loadPipelineCache = _1_loadPipelineCache_;

extern fn _1_savePipelineCache_(pRenderer: [*c]Renderer, pPipelineCache: [*c]PipelineCache, pDesc: [*c]PipelineCacheSaveDesc) void;
pub const savePipelineCache = _1_savePipelineCache_;

extern fn _1_isUma_() bool;
/// Determines whether we are using Uniform Memory Architecture or not.
/// Do not assume this variable will be the same, if code was compiled with multiple APIs result of this function might change per API.
pub const isUma = _1_isUma_;

// opaques

const Material = anyopaque;
